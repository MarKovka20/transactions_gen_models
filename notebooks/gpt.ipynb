{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.preprocessing import preprocess\n",
    "from src.utils.create_trainer import create_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../config\", version_base=None):\n",
    "    cfg = compose(config_name=\"master\")\n",
    "    \n",
    "# cfg_preprop = cfg[\"dataset\"]\n",
    "# cfg_model = cfg[\"model\"]\n",
    "backbone_cfg = cfg[\"backbone\"]\n",
    "logger_cfg = cfg[\"logger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(cfg[\"preprocessing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data\n",
    "\n",
    "train_data: Dataset = instantiate(\n",
    "    backbone_cfg[\"dataset\"], data=train, deterministic=False\n",
    ")\n",
    "val_data: Dataset = instantiate(\n",
    "    backbone_cfg[\"dataset\"], data=val, deterministic=True\n",
    ")\n",
    "test_data: Dataset = instantiate(\n",
    "    backbone_cfg[\"dataset\"], data=test, deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule: PtlsDataModule = instantiate(\n",
    "    {\n",
    "        '_target_': 'ptls.frames.PtlsDataModule', \n",
    "        'train_batch_size': 512, \n",
    "        'train_num_workers': 4, \n",
    "        'valid_batch_size': 512, \n",
    "        'valid_num_workers': 4\n",
    "    },\n",
    "    train_data=train_data,\n",
    "    valid_data=val_data,\n",
    "    test_data=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, Optional, Union\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from pytorch_lightning import LightningModule\n",
    "from sklearn import multiclass\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from torchmetrics import (\n",
    "    AUROC,\n",
    "    Accuracy,\n",
    "    AveragePrecision,\n",
    "    F1Score,\n",
    "    Metric,\n",
    "    MetricCollection,\n",
    "    MultitaskWrapper,\n",
    "    R2Score,\n",
    ")\n",
    "from torchmetrics.functional import auroc, f1_score, r2_score, average_precision\n",
    "\n",
    "from ptls.data_load import PaddedBatch\n",
    "from ptls.nn.seq_encoder.containers import SeqEncoderContainer\n",
    "\n",
    "from pytorch_lightning.utilities.types import LRSchedulerTypeUnion\n",
    "from src.nn.decoders.base import AbsDecoder\n",
    "from src.utils.logging_utils import get_logger\n",
    "\n",
    "\n",
    "class GPTModule(LightningModule):\n",
    "    \"\"\"A vanilla autoencoder, without masking, just encodes target sequence and then restores it.\n",
    "    Logs train/val/test losses:\n",
    "     - a CrossEntropyLoss on mcc codes\n",
    "     - an MSELoss on amounts\n",
    "    and train/val/test metrics:\n",
    "     - a macro-averaged multiclass f1-score on mcc codes\n",
    "     - a macro-averaged multiclass auroc score on mcc codes\n",
    "     - an r2-score on amounts\n",
    "\n",
    "     Attributes:\n",
    "        out_amount (nn.Linear):\n",
    "            A linear layer, which restores the transaction amounts.\n",
    "        out_mcc (nn.Linear):\n",
    "            A linear layer, which restores the transaction mcc codes.\n",
    "        amount_loss_weight (float):\n",
    "            Normalized loss weight for the transaction amount MSE loss.\n",
    "        mcc_loss_weight (float):\n",
    "            Normalized loss weight for the transaction mcc code CE loss.\n",
    "        lr (float):\n",
    "            The learning rate, extracted from the optimizer_config.\n",
    "        ae_output_size (int):\n",
    "            The output size of the decoder.\n",
    "\n",
    "    Notes:\n",
    "        amount_loss_weight, mcc_loss_weight are normalized so that amount_loss_weight + mcc_loss_weight = 1.\n",
    "        This is done to remove one hyperparameter. Loss gradient size can be managed separately through lr.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_weights: dict[Literal[\"amount\", \"mcc\"], float],\n",
    "        seq_encoder: DictConfig,\n",
    "        mcc_head: DictConfig,\n",
    "        amount_head: DictConfig,\n",
    "        optimizer: DictConfig, \n",
    "        scheduler: Optional[DictConfig] = None,\n",
    "        scheduler_config: Optional[dict] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize VanillaAE internal state.\n",
    "\n",
    "        Args:\n",
    "            loss_weights (dict):\n",
    "                A dictionary with keys \"amount\" and \"mcc\", mapping them to the corresponding loss weights\n",
    "            encoder (SeqEncoderContainer):\n",
    "                SeqEncoderContainer to be used as an encoder.\n",
    "            mcc_head (DictConfig):\n",
    "                DictConfig for mcc head, instantiated with in_channels keyword argument.\n",
    "            amount_head (DictConfig):\n",
    "                Partial dictconfig for amount head, instantiated with in_channels keyword argument.\n",
    "            optimizer (DictConfig):\n",
    "                Optimizer dictconfig, instantiated with params kwarg.\n",
    "            decoder (AbsDecoder):\n",
    "                AbsDecoder, to be used as the decoder.\n",
    "            scheduler (Optional[DictConfig]):\n",
    "                Optionally, an lr scheduler dictconfig, instantiated with optimizer kwarg\n",
    "            scheduler_config (Optional[dict]):\n",
    "                An lr_scheduler config for specifying scheduler-specific params, such as which metric to monitor\n",
    "                See LightningModule.configure_optimizers docstring for more details.\n",
    "            encoder_weights (Optional[str], optional):\n",
    "                Path to encoder weights. If not specified, no weights are loaded by default.\n",
    "            decoder_weights (Optional[str], optional):\n",
    "                Path to decoder weights. If not specified, no weights are loaded by default.\n",
    "            unfreeze_enc_after (Optional[int], optional):\n",
    "                Number of epochs to wait before unfreezing encoder weights.\n",
    "                The module doesn't get frozen by default.\n",
    "                A negative number would freeze the weights indefinetly.\n",
    "            unfreeze_dec_after (Optional[int], optional):\n",
    "                Number of epochs to wait before unfreezing encoder weights.\n",
    "                The module doesn't get frozen by default.\n",
    "                A negative number would freeze the weights indefinetly.\n",
    "            reconstruction_len (Optional[int]):\n",
    "                length of reconstructed batch in predict_step, optional.\n",
    "                If None, determine length from batch.seq_lens.\n",
    "                If int, reconstruct that many tokens.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.seq_encoder: SeqEncoderContainer = instantiate(seq_encoder)\n",
    "\n",
    "        self.amount_head = instantiate(amount_head, in_channels=self.seq_encoder.embedding_size)\n",
    "\n",
    "        self.mcc_head = instantiate(mcc_head, in_channels=self.seq_encoder.embedding_size)\n",
    "\n",
    "        self.optimizer_dictconfig = optimizer\n",
    "        self.scheduler_dictconfig = scheduler\n",
    "        self.scheduler_config = scheduler_config or {}\n",
    "\n",
    "        self.amount_loss_weight = loss_weights[\"amount\"] / sum(loss_weights.values())\n",
    "        self.mcc_loss_weight = loss_weights[\"mcc\"] / sum(loss_weights.values())\n",
    "\n",
    "        self.mcc_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.amount_criterion = nn.MSELoss()\n",
    "\n",
    "        multiclass_args: dict[str, Any] = dict(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.mcc_head[-2].out_features,\n",
    "            ignore_index=0,\n",
    "        )\n",
    "\n",
    "        MetricsType = dict[Literal[\"mcc\", \"amount\"], MetricCollection]\n",
    "        def make_metrics(stage: str) -> MetricsType:\n",
    "            return nn.ModuleDict({\n",
    "                \"mcc\": MetricCollection(\n",
    "                    AUROC(**multiclass_args, average=\"weighted\"),\n",
    "                    F1Score(**multiclass_args, average=\"micro\"),\n",
    "                    AveragePrecision(**multiclass_args, average=\"weighted\"),\n",
    "                    Accuracy(**multiclass_args, average=\"micro\"),\n",
    "                    prefix=stage\n",
    "                ),\n",
    "                \"amount\": MetricCollection(R2Score(), prefix=stage),\n",
    "            }) # type: ignore\n",
    "\n",
    "        self.train_metrics: MetricsType = make_metrics(\"train\")\n",
    "        self.val_metrics: MetricsType = make_metrics(\"val\")\n",
    "        self.test_metrics: MetricsType = make_metrics(\"test\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_encoder(x)\n",
    "\n",
    "    @property\n",
    "    def metric_name(self):\n",
    "        return \"val_loss\"\n",
    "\n",
    "    def _calculate_losses(\n",
    "        self,\n",
    "        mcc_pred: Tensor,\n",
    "        amount_pred: Tensor,\n",
    "        mcc_target: Tensor,\n",
    "        amount_target: Tensor,\n",
    "        mask: Tensor,\n",
    "    ) -> dict[str, Tensor]:\n",
    "        \"\"\"Calculate the losses, weigh them with respective weights\n",
    "\n",
    "        Args:\n",
    "            mcc_pred (Tensor): Predicted mcc logits, (B, L, mcc_vocab_size).\n",
    "            amount_pred (Tensor): Predicted amounts, (B, L).\n",
    "            mcc_target (Tensor): target mcc codes.\n",
    "            amount_target (Tensor): target amounts.\n",
    "            mask (Tensor): mask of non-padding elements\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of losses, with keys loss, loss_mcc, loss_amt.\n",
    "        \"\"\"\n",
    "        mcc_loss = self.mcc_criterion(mcc_pred[mask], mcc_target[mask])\n",
    "        amount_loss = self.amount_criterion(amount_pred[mask], amount_target[mask])\n",
    "\n",
    "        total_loss = (\n",
    "            self.mcc_loss_weight * mcc_loss + self.amount_loss_weight * amount_loss\n",
    "        )\n",
    "\n",
    "        return {\"loss\": total_loss, \"loss_mcc\": mcc_loss, \"loss_amt\": amount_loss}\n",
    "\n",
    "    def shared_step(\n",
    "        self,\n",
    "        stage: Literal[\"train\", \"val\", \"test\"],\n",
    "        batch: PaddedBatch,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> STEP_OUTPUT:\n",
    "        \"\"\"Generalized function to do a train/val/test step.\n",
    "\n",
    "        Args:\n",
    "            stage (str): train, val, or test, depending on the stage.\n",
    "            batch (PaddedBatch): Input.\n",
    "            batch_idx (int): ignored\n",
    "\n",
    "        Returns:\n",
    "            STEP_OUTPUT:\n",
    "                if stage == \"train\", returns total loss.\n",
    "                else returns a dictionary of metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings = self(batch).payload\n",
    "\n",
    "        mcc_pred = self.mcc_head(embeddings)[:, :-1, :]\n",
    "        amount_pred = self.amount_head(embeddings)[:, :-1].squeeze(-1)\n",
    "        \n",
    "        mcc_target = batch.payload[\"mcc_code\"][:, 1:]\n",
    "        amount_target = torch.log(batch.payload[\"amount\"][:, 1:] + 1)  # Logarithmize targets\n",
    "\n",
    "        nonpad_mask = batch.seq_len_mask[:, 1:].bool()\n",
    "\n",
    "        loss_dict = self._calculate_losses(\n",
    "            mcc_pred, amount_pred, mcc_target, amount_target, nonpad_mask\n",
    "        )\n",
    "\n",
    "        metrics = {\n",
    "            \"train\": self.train_metrics,\n",
    "            \"val\": self.val_metrics,\n",
    "            \"test\": self.test_metrics,\n",
    "        }[stage]\n",
    "\n",
    "        metrics[\"mcc\"].update(mcc_pred[nonpad_mask], mcc_target[nonpad_mask])\n",
    "        metrics[\"amount\"].update(amount_pred[nonpad_mask], amount_target[nonpad_mask])\n",
    "\n",
    "        self.log_dict(\n",
    "            {f\"{stage}_{k}\": v for k, v in loss_dict.items()},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            batch_size=batch.seq_feature_shape[0],\n",
    "        )\n",
    "        \n",
    "        for metric in metrics.values():\n",
    "            self.log_dict(\n",
    "                metric, # type: ignore\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                batch_size=batch.seq_feature_shape[0],\n",
    "            )\n",
    "\n",
    "        return loss_dict[\"loss\"]\n",
    "\n",
    "    def training_step(self, *args, **kwargs) -> STEP_OUTPUT:\n",
    "        return self.shared_step(\"train\", *args, **kwargs)\n",
    "\n",
    "    def validation_step(self, *args, **kwargs) -> Union[STEP_OUTPUT, None]:\n",
    "        return self.shared_step(\"val\", *args, **kwargs)\n",
    "\n",
    "    def test_step(self, *args, **kwargs) -> Union[STEP_OUTPUT, None]:\n",
    "        return self.shared_step(\"test\", *args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = instantiate(self.optimizer_dictconfig, params=self.parameters())\n",
    "\n",
    "        if self.scheduler_dictconfig:\n",
    "            scheduler = instantiate(self.scheduler_dictconfig, optimizer=optimizer)\n",
    "            scheduler_config = {\"scheduler\": scheduler, **self.scheduler_config}\n",
    "\n",
    "            return [optimizer], [scheduler_config]\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    # Overriding lr_scheduler_step to fool the exception (which doesn't appear in later versions of pytorch_lightning):\n",
    "    # pytorch_lightning.utilities.exceptions.MisconfigurationException:\n",
    "    #   The provided lr scheduler `...` doesn't follow PyTorch's LRScheduler API.\n",
    "    #   You should override the `LightningModule.lr_scheduler_step` hook with your own logic if you are using a custom LR scheduler.\n",
    "    def lr_scheduler_step(\n",
    "        self, scheduler: LRSchedulerTypeUnion, optimizer_idx: int, metric\n",
    "    ) -> None:\n",
    "        return super().lr_scheduler_step(scheduler, optimizer_idx, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.bert.losses.query_soft_max import QuerySoftmaxLoss\n",
    "\n",
    "class GPTContrastiveModule(LightningModule):\n",
    "    \"\"\"A vanilla autoencoder, without masking, just encodes target sequence and then restores it.\n",
    "    Logs train/val/test losses:\n",
    "     - a CrossEntropyLoss on mcc codes\n",
    "     - an MSELoss on amounts\n",
    "    and train/val/test metrics:\n",
    "     - a macro-averaged multiclass f1-score on mcc codes\n",
    "     - a macro-averaged multiclass auroc score on mcc codes\n",
    "     - an r2-score on amounts\n",
    "\n",
    "     Attributes:\n",
    "        out_amount (nn.Linear):\n",
    "            A linear layer, which restores the transaction amounts.\n",
    "        out_mcc (nn.Linear):\n",
    "            A linear layer, which restores the transaction mcc codes.\n",
    "        amount_loss_weight (float):\n",
    "            Normalized loss weight for the transaction amount MSE loss.\n",
    "        mcc_loss_weight (float):\n",
    "            Normalized loss weight for the transaction mcc code CE loss.\n",
    "        lr (float):\n",
    "            The learning rate, extracted from the optimizer_config.\n",
    "        ae_output_size (int):\n",
    "            The output size of the decoder.\n",
    "\n",
    "    Notes:\n",
    "        amount_loss_weight, mcc_loss_weight are normalized so that amount_loss_weight + mcc_loss_weight = 1.\n",
    "        This is done to remove one hyperparameter. Loss gradient size can be managed separately through lr.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: DictConfig,\n",
    "        optimizer: DictConfig, \n",
    "        scheduler: Optional[DictConfig] = None,\n",
    "        scheduler_config: Optional[dict] = None,\n",
    "        neg_count: int = 5,\n",
    "        temperature: float = 20,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize VanillaAE internal state.\n",
    "\n",
    "        Args:\n",
    "            loss_weights (dict):\n",
    "                A dictionary with keys \"amount\" and \"mcc\", mapping them to the corresponding loss weights\n",
    "            encoder (SeqEncoderContainer):\n",
    "                SeqEncoderContainer to be used as an encoder.\n",
    "            mcc_head (DictConfig):\n",
    "                DictConfig for mcc head, instantiated with in_channels keyword argument.\n",
    "            amount_head (DictConfig):\n",
    "                Partial dictconfig for amount head, instantiated with in_channels keyword argument.\n",
    "            optimizer (DictConfig):\n",
    "                Optimizer dictconfig, instantiated with params kwarg.\n",
    "            decoder (AbsDecoder):\n",
    "                AbsDecoder, to be used as the decoder.\n",
    "            scheduler (Optional[DictConfig]):\n",
    "                Optionally, an lr scheduler dictconfig, instantiated with optimizer kwarg\n",
    "            scheduler_config (Optional[dict]):\n",
    "                An lr_scheduler config for specifying scheduler-specific params, such as which metric to monitor\n",
    "                See LightningModule.configure_optimizers docstring for more details.\n",
    "            encoder_weights (Optional[str], optional):\n",
    "                Path to encoder weights. If not specified, no weights are loaded by default.\n",
    "            decoder_weights (Optional[str], optional):\n",
    "                Path to decoder weights. If not specified, no weights are loaded by default.\n",
    "            unfreeze_enc_after (Optional[int], optional):\n",
    "                Number of epochs to wait before unfreezing encoder weights.\n",
    "                The module doesn't get frozen by default.\n",
    "                A negative number would freeze the weights indefinetly.\n",
    "            unfreeze_dec_after (Optional[int], optional):\n",
    "                Number of epochs to wait before unfreezing encoder weights.\n",
    "                The module doesn't get frozen by default.\n",
    "                A negative number would freeze the weights indefinetly.\n",
    "            reconstruction_len (Optional[int]):\n",
    "                length of reconstructed batch in predict_step, optional.\n",
    "                If None, determine length from batch.seq_lens.\n",
    "                If int, reconstruct that many tokens.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder: SeqEncoderContainer = instantiate(encoder)\n",
    "        self.head = instantiate({\"_target_\": \"torchvision.ops.MLP\"}, in_channels=self.encoder.embedding_size, hidden_channels=[self.encoder.trx_encoder.output_size])\n",
    "\n",
    "        self.optimizer_dictconfig = optimizer\n",
    "        self.scheduler_dictconfig = scheduler\n",
    "        self.scheduler_config = scheduler_config or {}\n",
    "\n",
    "        self.loss_fn = QuerySoftmaxLoss(temperature, reduce=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def get_neg_ix(self, mask):\n",
    "        \"\"\"Sample from predicts, where `mask == True`, without self element.\n",
    "        sample from predicted tokens from batch\n",
    "        \"\"\"\n",
    "        mask_num = mask.int().sum()\n",
    "        mn = 1 - torch.eye(mask_num, device=mask.device)\n",
    "        neg_ix = torch.multinomial(mn, self.hparams.neg_count)\n",
    "\n",
    "        b_ix = torch.arange(mask.size(0), device=mask.device).view(-1, 1).expand_as(mask)[mask][neg_ix]\n",
    "        t_ix = torch.arange(mask.size(1), device=mask.device).view(1, -1).expand_as(mask)[mask][neg_ix]\n",
    "        return b_ix, t_ix\n",
    "\n",
    "    @property\n",
    "    def metric_name(self):\n",
    "        return \"val_loss\"\n",
    "\n",
    "    def shared_step(\n",
    "        self,\n",
    "        stage,\n",
    "        batch: PaddedBatch,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> STEP_OUTPUT:\n",
    "        \"\"\"Generalized function to do a train/val/test step.\n",
    "\n",
    "        Args:\n",
    "            stage (str): train, val, or test, depending on the stage.\n",
    "            batch (PaddedBatch): Input.\n",
    "            batch_idx (int): ignored\n",
    "\n",
    "        Returns:\n",
    "            STEP_OUTPUT:\n",
    "                if stage == \"train\", returns total loss.\n",
    "                else returns a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        mask = batch.seq_len_mask[:, 1:].bool()\n",
    "        x_trx = self.encoder.trx_encoder(batch).payload[:, 1:]\n",
    "\n",
    "        embeddings = self.encoder(batch).payload[:, :-1, :]\n",
    "        out = self.head(embeddings)\n",
    "\n",
    "        target = x_trx[mask].unsqueeze(1)  # N, 1, H\n",
    "        predict = out[mask].unsqueeze(1)  # N, 1, H\n",
    "\n",
    "        neg_ix = self.get_neg_ix(mask)\n",
    "        negative = out[neg_ix[0], neg_ix[1]]  # N, nneg, H\n",
    "\n",
    "        loss = self.loss_fn(target, predict, negative)\n",
    "\n",
    "        self.log_dict(\n",
    "            {f\"{stage}_loss\": loss.item()},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            batch_size=batch.seq_feature_shape[0],\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, *args, **kwargs) -> STEP_OUTPUT:\n",
    "        return self.shared_step(\"train\", *args, **kwargs)\n",
    "\n",
    "    def validation_step(self, *args, **kwargs) -> Union[STEP_OUTPUT, None]:\n",
    "        return self.shared_step(\"val\", *args, **kwargs)\n",
    "\n",
    "    def test_step(self, *args, **kwargs) -> Union[STEP_OUTPUT, None]:\n",
    "        return self.shared_step(\"test\", *args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = instantiate(self.optimizer_dictconfig, params=self.parameters())\n",
    "\n",
    "        if self.scheduler_dictconfig:\n",
    "            scheduler = instantiate(self.scheduler_dictconfig, optimizer=optimizer)\n",
    "            scheduler_config = {\"scheduler\": scheduler, **self.scheduler_config}\n",
    "\n",
    "            return [optimizer], [scheduler_config]\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    # Overriding lr_scheduler_step to fool the exception (which doesn't appear in later versions of pytorch_lightning):\n",
    "    # pytorch_lightning.utilities.exceptions.MisconfigurationException:\n",
    "    #   The provided lr scheduler `...` doesn't follow PyTorch's LRScheduler API.\n",
    "    #   You should override the `LightningModule.lr_scheduler_step` hook with your own logic if you are using a custom LR scheduler.\n",
    "    def lr_scheduler_step(\n",
    "        self, scheduler: LRSchedulerTypeUnion, optimizer_idx: int, metric\n",
    "    ) -> None:\n",
    "        return super().lr_scheduler_step(scheduler, optimizer_idx, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_args = {}\n",
    "module_args[\"encoder\"] = backbone_cfg[\"encoder\"]\n",
    "if \"decoder\" in backbone_cfg:\n",
    "    module_args[\"decoder\"] = backbone_cfg[\"decoder\"]\n",
    "\n",
    "# Instantiate the LightningModule.\n",
    "# _recursive_=False to save all hyperparameters\n",
    "# as DictConfigs, to enable hp loading from lightning checkpoint\n",
    "module = instantiate(\n",
    "    backbone_cfg[\"module\"], **module_args, _recursive_=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = GPTContrastiveModule(\n",
    "    encoder={\n",
    "        '_target_': 'ptls.nn.RnnSeqEncoder', \n",
    "        'trx_encoder': {\n",
    "            '_target_': 'ptls.nn.TrxEncoder', \n",
    "            'use_batch_norm_with_lens': True, \n",
    "            'norm_embeddings': False, \n",
    "            'embeddings_noise': 0.0003, \n",
    "            'embeddings': {\n",
    "                'mcc_code': {'in': 344, 'out': 24}}, \n",
    "                'numeric_values': {'amount': 'log'}\n",
    "            }, \n",
    "        'hidden_size': 1024, \n",
    "        'bidir': False, \n",
    "        'trainable_starter': 'static', \n",
    "        'type': 'lstm', \n",
    "        'is_reduce_sequence': False\n",
    "    },\n",
    "    # head={'_target_': 'torchvision.ops.MLP', 'hidden_channels': [25]},\n",
    "    # amount_head={'_target_': 'torchvision.ops.MLP', 'hidden_channels': [1]},\n",
    "    optimizer={'_target_': 'torch.optim.AdamW', 'lr': 0.0005},\n",
    "    scheduler={'_target_': 'torch.optim.lr_scheduler.StepLR', 'step_size': 800, 'gamma': 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | RnnSeqEncoder    | 4.3 M \n",
      "1 | head    | MLP              | 25.6 K\n",
      "2 | loss_fn | QuerySoftmaxLoss | 0     \n",
      "---------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "8.680     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1b9f1b498f4199bc54358657b66854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macro-micro-coles/miniconda/envs/env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1933: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c1f23a9b884db38816cc40efae218d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cf0b0f1fb74e33a1f008aa22a30270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65aefa5dc1cd40d39f9255cf653e5140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a28e1f53e7a43a2a9eb660db8a8f94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3e117d43054898b2445e67a4cb14b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd467749f54d51a9d24c61a3f69323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macro-micro-coles/miniconda/envs/env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = create_trainer(\n",
    "    metric_name=module.metric_name,\n",
    "    **backbone_cfg[\"trainer\"],\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.fit(module, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(module.seq_encoder.state_dict(), \"gpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.global_validation.global_validation_pipeline import global_target_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_data': {'batch_size': 64, 'device': 'cuda'}, 'n_runs': '${oc.decode:${oc.env:FAST_DEV_RUN,10}}', 'model': {'_target_': 'lightgbm.LGBMClassifier', 'n_estimators': 500, 'boosting_type': 'gbdt', 'subsample': 0.5, 'subsample_freq': 1, 'learning_rate': 0.02, 'feature_fraction': 0.75, 'max_depth': 6, 'lambda_l1': 1, 'lambda_l2': 1, 'min_data_in_leaf': 50, 'random_state': 42, 'n_jobs': 8, 'verbose': -1}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = global_target_validation(\n",
    "    data, \n",
    "    {\n",
    "        '_target_': 'ptls.nn.RnnSeqEncoder', \n",
    "        'trx_encoder': {\n",
    "            '_target_': 'ptls.nn.TrxEncoder', \n",
    "            'use_batch_norm_with_lens': True, \n",
    "            'norm_embeddings': False, \n",
    "            'embeddings_noise': 0.0003, \n",
    "            'embeddings': {\n",
    "                'mcc_code': {'in': 344, 'out': 24}}, \n",
    "                'numeric_values': {'amount': 'log'}\n",
    "            }, \n",
    "        'hidden_size': 1024, \n",
    "        'bidir': False, \n",
    "        'trainable_starter': 'static', \n",
    "        'type': 'lstm', \n",
    "        'is_reduce_sequence': True\n",
    "    },\n",
    "    cfg[\"validation\"][\"global_target\"],\n",
    "    \"gpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUROC</th>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.721238</td>\n",
       "      <td>0.748357</td>\n",
       "      <td>0.66720</td>\n",
       "      <td>0.721139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011553</td>\n",
       "      <td>0.010213</td>\n",
       "      <td>0.01255</td>\n",
       "      <td>0.012974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AUROC    PR-AUC  Accuracy   F1Score\n",
       "mean  0.721238  0.748357   0.66720  0.721139\n",
       "std   0.011553  0.010213   0.01255  0.012974"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macro-micro-coles/miniconda/envs/env/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:374: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(12.3183, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.shared_step(\"train\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaAE(\n",
       "  (encoder): RnnSeqEncoder(\n",
       "    (trx_encoder): TrxEncoder(\n",
       "      (embeddings): ModuleDict(\n",
       "        (mcc_code): NoisyEmbedding(\n",
       "          344, 24, padding_idx=0\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (numeric_values): ModuleDict(\n",
       "        (amount): LogScaler()\n",
       "      )\n",
       "      (numerical_batch_norm): RBatchNormWithLens(\n",
       "        (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (seq_encoder): RnnEncoder(\n",
       "      (rnn): LSTM(25, 1024, batch_first=True)\n",
       "      (reducer): LastStepEncoder()\n",
       "    )\n",
       "  )\n",
       "  (decoder): LSTMCellDecoder(\n",
       "    (cell): LSTMCell(1024, 2048)\n",
       "    (projector): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (lstm): Identity()\n",
       "  )\n",
       "  (amount_head): MLP(\n",
       "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mcc_head): MLP(\n",
       "    (0): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mcc_criterion): CrossEntropyLoss()\n",
       "  (amount_criterion): MSELoss()\n",
       "  (train_metrics): ModuleDict(\n",
       "    (mcc): MetricCollection(\n",
       "      (MulticlassAUROC): MulticlassAUROC()\n",
       "      (MulticlassF1Score): MulticlassF1Score()\n",
       "      (MulticlassAveragePrecision): MulticlassAveragePrecision()\n",
       "      (MulticlassAccuracy): MulticlassAccuracy(),\n",
       "      prefix=train\n",
       "    )\n",
       "    (amount): MetricCollection(\n",
       "      (R2Score): R2Score(),\n",
       "      prefix=train\n",
       "    )\n",
       "  )\n",
       "  (val_metrics): ModuleDict(\n",
       "    (mcc): MetricCollection(\n",
       "      (MulticlassAUROC): MulticlassAUROC()\n",
       "      (MulticlassF1Score): MulticlassF1Score()\n",
       "      (MulticlassAveragePrecision): MulticlassAveragePrecision()\n",
       "      (MulticlassAccuracy): MulticlassAccuracy(),\n",
       "      prefix=val\n",
       "    )\n",
       "    (amount): MetricCollection(\n",
       "      (R2Score): R2Score(),\n",
       "      prefix=val\n",
       "    )\n",
       "  )\n",
       "  (test_metrics): ModuleDict(\n",
       "    (mcc): MetricCollection(\n",
       "      (MulticlassAUROC): MulticlassAUROC()\n",
       "      (MulticlassF1Score): MulticlassF1Score()\n",
       "      (MulticlassAveragePrecision): MulticlassAveragePrecision()\n",
       "      (MulticlassAccuracy): MulticlassAccuracy(),\n",
       "      prefix=test\n",
       "    )\n",
       "    (amount): MetricCollection(\n",
       "      (R2Score): R2Score(),\n",
       "      prefix=test\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>mcc_code</th>\n",
       "      <th>amount</th>\n",
       "      <th>global_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33172</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13.887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33172</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>15.983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33172</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>21.341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33172</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>17.941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33172</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>17.726</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33172</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>47.397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33172</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>220.009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33172</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9.067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  timestamp  mcc_code   amount  global_target\n",
       "0    33172          6         4   71.463              0\n",
       "1    33172          6        35   45.017              0\n",
       "2    33172          8        11   13.887              0\n",
       "3    33172          9        11   15.983              0\n",
       "4    33172         10        11   21.341              0\n",
       "5    33172         11        11   17.941              0\n",
       "6    33172         12        11   17.726              0\n",
       "7    33172         13        18   47.397              0\n",
       "8    33172         13         1  220.009              0\n",
       "9    33172         13        11    9.067              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(Path(cfg[\"dataset\"][\"dir_path\"]).joinpath(cfg[\"dataset\"][\"train_file_name\"]))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id=\"user_id\",\n",
    "    col_event_time=\"timestamp\",\n",
    "    event_time_transformation=\"none\",\n",
    "    cols_category=[\"mcc_code\"],\n",
    "    cols_numerical=[\"amount\"],\n",
    "    return_records=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(dataset, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data: CustomColesDataset = instantiate(cfg_model[\"dataset\"], data=train)\n",
    "val_data: CustomColesDataset = instantiate(cfg_model[\"dataset\"], data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule: PtlsDataModule = instantiate(\n",
    "    cfg_model[\"datamodule\"],\n",
    "    train_data=train_data,\n",
    "    valid_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: CustomCoLES = instantiate(cfg_model[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint: ModelCheckpoint = instantiate(\n",
    "    cfg_model[\"trainer_coles\"][\"checkpoint_callback\"],\n",
    "    monitor=model.metric_name,\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping: EarlyStopping = instantiate(\n",
    "    cfg_model[\"trainer_coles\"][\"early_stopping\"],\n",
    "    monitor=model.metric_name,\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger: TensorBoardLogger = instantiate(cfg_model[\"trainer_coles\"][\"logger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer: Trainer = instantiate(\n",
    "    cfg_model[\"trainer_coles\"][\"trainer\"],\n",
    "    callbacks=[model_checkpoint, early_stopping],\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/miniconda3/envs/micro_macro_coles/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /home/nikita/Documents/work/macro_micro_coles/logs/checkpoints/coles exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 18.5 K\n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "18.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.5 K    Total params\n",
      "0.074     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5357132ee5fc4e62bfc6776be8d060f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ff303a48bd4ee99648e2ac88c8fd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/miniconda3/envs/micro_macro_coles/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d85f8d091281e8cdbbabc30d35abdaebd4eb091dd84d434d9fa6db6a3942f76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
