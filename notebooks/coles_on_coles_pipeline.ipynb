{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.frames import PtlsDataModule\n",
    "import ptls\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.coles import CustomColesDataset, CustomCoLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "\n",
    "from ptls.nn.seq_encoder.containers import SeqEncoderContainer\n",
    "from ptls.frames.coles import CoLESModule\n",
    "\n",
    "\n",
    "class CustomCoLES(CoLESModule):\n",
    "    \"\"\"\n",
    "    Custom coles module inhereted from ptls coles module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer_partial: Callable,\n",
    "        lr_scheduler_partial: Callable,\n",
    "        sequence_encoder: SeqEncoderContainer,\n",
    "    ) -> None:\n",
    "        \"\"\"Overrided initialize method, which is suitable for our tasks\n",
    "\n",
    "        Args:\n",
    "            optimizer_partial (Callable): Partial initialized torch optimizer (with parameters)\n",
    "            lr_scheduler_partial (Callable): Partial initialized torch lr scheduler\n",
    "                (with parameters)\n",
    "            sequence_encoder (SeqEncoderContainer): Ptls sequence encoder\n",
    "                (including sequence encoder and single transaction encoder)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            seq_encoder=sequence_encoder,\n",
    "            optimizer_partial=optimizer_partial,\n",
    "            lr_scheduler_partial=lr_scheduler_partial,\n",
    "        )\n",
    "        self.sequence_encoder_model = sequence_encoder\n",
    "\n",
    "    def get_seq_encoder_weights(self) -> Dict:\n",
    "        \"\"\"Get weights of the sequnce encoder in torch format\n",
    "\n",
    "        Returns:\n",
    "            dict: Encoder weights\n",
    "        \"\"\"\n",
    "        return self.sequence_encoder_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from ptls.nn.head import Head\n",
    "from src.coles.datamodule import SampleAll\n",
    "from ptls.frames.coles.losses import ContrastiveLoss\n",
    "from ptls.frames.coles.metric import BatchRecallTopK\n",
    "from ptls.frames.coles.sampling_strategies import HardNegativePairSelector\n",
    "from ptls.frames.coles.split_strategy import AbsSplit, SampleSlices\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import minimum, maximum\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from ptls.nn.seq_encoder.abs_seq_encoder import AbsSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from torchmetrics import Metric\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def is_seq_feature(k: str, x):\n",
    "    \"\"\"Check is value sequential feature\n",
    "    Synchronized with ptls.data_load.padded_batch.PaddedBatch.is_seq_feature\n",
    "\n",
    "    Iterables are:\n",
    "        np.array\n",
    "        torch.Tensor\n",
    "\n",
    "    Not iterable:\n",
    "        list    - dont supports indexing\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k:\n",
    "        feature_name\n",
    "    x:\n",
    "        value for check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        True if value is iterable\n",
    "    \"\"\"\n",
    "    if k == 'event_time':\n",
    "        return True\n",
    "    if k.startswith('target'):\n",
    "        return False\n",
    "    if type(x) in (np.ndarray, torch.Tensor):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class CoLESonCoLES(CoLESModule):\n",
    "    \"\"\"\n",
    "    Coles on coles embeddings model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frozen_encoder: SeqEncoderContainer,\n",
    "        learning_encoder: AbsSeqEncoder,\n",
    "        optimizer_partial: Callable,\n",
    "        lr_scheduler_partial: Callable,\n",
    "        col_time: str = \"event_time\",\n",
    "        encoding_splitter: AbsSplit = SampleAll(20, 1),\n",
    "        training_splitter: AbsSplit = SampleSlices(split_count=5, cnt_max=150, cnt_min=15)\n",
    "    ) -> None:\n",
    "        \"\"\" \n",
    "        something should be placed here\n",
    "        \"\"\"\n",
    "        super().__init__(seq_encoder=frozen_encoder,\n",
    "                         optimizer_partial=optimizer_partial,\n",
    "                         lr_scheduler_partial=lr_scheduler_partial)\n",
    "    \n",
    "        self.learning_encoder = learning_encoder\n",
    "\n",
    "        self.encoding_splitter = encoding_splitter\n",
    "        self.training_splitter = training_splitter\n",
    "        self.col_time = col_time \n",
    "\n",
    "    @property\n",
    "    def is_requires_reduced_sequence(self):\n",
    "        return False\n",
    "    \n",
    "    def _encode_and_split(self, x: PaddedBatch, y: torch.Tensor, step: int = 20):\n",
    "        dates = x.payload[self.col_time]   # (B, T)\n",
    "        dates_len = dates.shape[1]\n",
    "        start_pos = np.arange(0, dates_len - step, 1)\n",
    "\n",
    "        def encode():\n",
    "            for s in start_pos:\n",
    "                torch.cuda.empty_cache()\n",
    "                payload = {k: v[:, s : s + step] for k, v in x.payload.items() if is_seq_feature(k, v)}\n",
    "                seq_lens = minimum(maximum(x.seq_lens - s, torch.zeros_like(x.seq_lens)), torch.full_like(x.seq_lens, step))\n",
    "                pb = PaddedBatch(payload, seq_lens)\n",
    "\n",
    "                yield self._seq_encoder(pb).payload.detach().cpu()\n",
    "\n",
    "        emb_sequences = torch.stack([*encode()], dim=1)\n",
    "\n",
    "        def split():\n",
    "            for i, elem in enumerate(emb_sequences):\n",
    "                indexes = self.training_splitter.split(dates)\n",
    "                yield [elem[ix] for ix in indexes], [y[i].item()] * len(indexes)\n",
    "\n",
    "        emb_sequences, extended_y = zip(*split())\n",
    "\n",
    "        emb_sequences = list(chain(*emb_sequences)) # B\n",
    "        emb_sequences = pad_sequence(emb_sequences, batch_first=True).to(x.device)\n",
    "\n",
    "        extended_y = torch.tensor(list(chain(*extended_y))).to(x.device)\n",
    "        \n",
    "        return PaddedBatch(payload=emb_sequences, length=x.seq_lens), extended_y\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.learning_encoder(x)\n",
    "    \n",
    "    def shared_step(self, x, y):\n",
    "        x, y = self._encode_and_split(x, y)\n",
    "        y_h = self(x)\n",
    "        y_h = self._head(y_h)\n",
    "        return y_h, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../config\", version_base=None):\n",
    "    cfg = compose(config_name=\"config_churn\")\n",
    "    \n",
    "cfg_preprop = cfg[\"dataset\"]\n",
    "cfg_model = cfg[\"model\"]\n",
    "cfg_model[\"trainer_coles\"][\"trainer\"][\"devices\"] = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>mcc_code</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>global_target</th>\n",
       "      <th>holiday_target</th>\n",
       "      <th>weekend_target</th>\n",
       "      <th>churn_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-10-16 00:00:00</td>\n",
       "      <td>380.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-10-10 00:00:00</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-10-16 00:00:00</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-10-11 00:00:00</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-07-26 00:00:00</td>\n",
       "      <td>598.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  mcc_code           timestamp   amount  global_target  \\\n",
       "0        0        19 2017-10-21 00:00:00   5023.0              0   \n",
       "1        0         2 2017-10-12 12:24:07  20000.0              0   \n",
       "2        0        10 2017-12-05 00:00:00    767.0              0   \n",
       "3        0         1 2017-10-21 00:00:00   2031.0              0   \n",
       "4        0         9 2017-10-24 13:14:24  36562.0              0   \n",
       "5        1         3 2017-10-16 00:00:00    380.0              0   \n",
       "6        1         3 2017-10-10 00:00:00    378.0              0   \n",
       "7        1         3 2017-10-16 00:00:00    199.0              0   \n",
       "8        1         3 2017-10-11 00:00:00    400.0              0   \n",
       "9        1         1 2017-07-26 00:00:00    598.0              0   \n",
       "\n",
       "   holiday_target  weekend_target  churn_target  \n",
       "0               0               1             0  \n",
       "1               0               0             0  \n",
       "2               0               0             0  \n",
       "3               0               1             0  \n",
       "4               0               0             0  \n",
       "5               0               0             0  \n",
       "6               0               0             0  \n",
       "7               0               0             0  \n",
       "8               0               0             0  \n",
       "9               0               0             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(Path(cfg[\"dataset\"][\"dir_path\"]).joinpath(cfg[\"dataset\"][\"train_file_name\"]))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id=\"user_id\",\n",
    "    col_event_time=\"timestamp\",\n",
    "    event_time_transformation=\"dt_to_timestamp\",\n",
    "    cols_category=[\"mcc_code\"],\n",
    "    cols_numerical=[\"amount\"],\n",
    "    return_records=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(dataset, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data: CustomColesDataset = instantiate(cfg_model[\"dataset\"], data=train, splitter = ptls.frames.coles.split_strategy.NoSplit())\n",
    "val_data: CustomColesDataset = instantiate(cfg_model[\"dataset\"], data=val, splitter=ptls.frames.coles.split_strategy.NoSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule: PtlsDataModule = instantiate(\n",
    "    cfg_model[\"datamodule\"],\n",
    "    train_data=train_data,\n",
    "    valid_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 350])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].payload[\"global_target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (b[0].to(\"cuda:0\"), b[1].to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: CustomCoLES = instantiate(cfg_model[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "frozen_encoder = model.seq_encoder.to(\"cuda:0\")\n",
    "learning_encoder = model.seq_encoder.seq_encoder.to(\"cuda:0\")\n",
    "\n",
    "optimizer = partial(torch.optim.Adam, lr=0.004)\n",
    "lr_scheduler = partial(torch.optim.lr_scheduler.ReduceLROnPlateau, factor=0.902, patience=2)\n",
    "mdl = CoLESonCoLES(frozen_encoder=frozen_encoder, learning_encoder=learning_encoder, optimizer_partial=optimizer, lr_scheduler_partial=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl._seq_encoder.is_reduce_sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "LSTM: Expected input to be 2-D or 3-D but received 4-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/app/notebooks/coles_on_coles_pipeline.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x, y \u001b[39m=\u001b[39m mdl\u001b[39m.\u001b[39;49mshared_step(\u001b[39m*\u001b[39;49mb)\n",
      "\u001b[1;32m/app/notebooks/coles_on_coles_pipeline.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshared_step\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_and_split(x, y)\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     y_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m     y_h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_head(y_h)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y_h, y\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/app/notebooks/coles_on_coles_pipeline.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f62617a61726f76612d636f6c657332222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d69646173227d7d/app/notebooks/coles_on_coles_pipeline.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_encoder(x)\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/ptls/nn/seq_encoder/rnn_encoder.py:132\u001b[0m, in \u001b[0;36mRnnEncoder.forward\u001b[0;34m(self, x, h_0)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m# pass-through rnn\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x\u001b[39m.\u001b[39;49mpayload)\n\u001b[1;32m    133\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgru\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    134\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(x\u001b[39m.\u001b[39mpayload, h_0)\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:773\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     batch_sizes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m     \u001b[39massert\u001b[39;00m (\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTM: Expected input to be 2-D or 3-D but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m    775\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: LSTM: Expected input to be 2-D or 3-D but received 4-D tensor"
     ]
    }
   ],
   "source": [
    "x, y = mdl.shared_step(*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  4 19:29:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 73%   61C    P2    86W / 170W |   4593MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 57%   56C    P2   137W / 170W |  11976MiB / 12288MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
