{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate, call\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "from src.local_validation.local_validation_model import LocalValidationModelBase\n",
    "from src.preprocessing import preprocess\n",
    "\n",
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "from src.datasets.coles import CustomColesDataset\n",
    "from src.modules.coles import CustomCoLES\n",
    "\n",
    "from src.pooling import PoolingModel\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from src.utils.create_trainer import create_trainer\n",
    "from typing import Union\n",
    "from src.modules import Cotic, CustomCoLES, TS2Vec, VanillaAE\n",
    "from pytorch_lightning.utilities.model_helpers import is_overridden\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import learn_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"master\"\n",
    "backbone_path = \"saved_models/ar_default_1.pth\"#\"saved_models/churn/gpt/gpt_churn_1.pth\"\n",
    "SEED = 1\n",
    "\n",
    "with initialize(config_path=\"../config\", version_base=None):\n",
    "    cfg = compose(config_name=config_name)\n",
    "cfg_preprop = cfg[\"preprocessing\"]\n",
    "cfg_encoder = cfg[\"backbone\"][\"encoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(train)[\"user_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆ                                                                                                                                                                      | 34/5664 [00:02<05:32, 16.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sequence_encoder \u001b[38;5;241m=\u001b[39m instantiate(cfg_encoder, is_reduce_sequence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m sequence_encoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(backbone_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m \u001b[43mPoolingModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbackbone_embd_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_users_in_train_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpooling_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearnable_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmin_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_embs_per_user\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/src/pooling/new_pooling_model.py:101\u001b[0m, in \u001b[0;36mPoolingModel.__init__\u001b[0;34m(self, train_data, backbone, backbone_embd_size, pooling_type, max_users_in_train_dataloader, min_seq_length, max_seq_length, max_embs_per_user, init_device, freeze, additional_config)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_embs_per_user \u001b[38;5;241m=\u001b[39m max_embs_per_user\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membegings_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_pooled_embegings_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_users_in_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_device\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pooling_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearnable_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearnable_attention_matrix \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_embd_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_embd_size\n\u001b[1;32m    108\u001b[0m     )\n",
      "File \u001b[0;32m/app/src/pooling/new_pooling_model.py:192\u001b[0m, in \u001b[0;36mPoolingModel.make_pooled_embegings_dataset\u001b[0;34m(self, train_data, max_users_in_train_dataloader, device)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    191\u001b[0m data\u001b[38;5;241m.\u001b[39mappend({})\n\u001b[0;32m--> 192\u001b[0m embs, times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data_for_one_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m all_times\u001b[38;5;241m.\u001b[39mupdate(times)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emb, time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embs, times):\n",
      "File \u001b[0;32m/app/src/pooling/new_pooling_model.py:157\u001b[0m, in \u001b[0;36mPoolingModel.prepare_data_for_one_user\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m    155\u001b[0m     resulting_user_data\u001b[38;5;241m.\u001b[39mappend(data_for_timestamp)\n\u001b[1;32m    156\u001b[0m resulting_user_data \u001b[38;5;241m=\u001b[39m collate_feature_dict(resulting_user_data)\n\u001b[0;32m--> 157\u001b[0m embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresulting_user_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mis_reduce_sequence:\n\u001b[1;32m    159\u001b[0m     embs \u001b[38;5;241m=\u001b[39m embs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/ptls/nn/seq_encoder/containers.py:125\u001b[0m, in \u001b[0;36mRnnSeqEncoder.forward\u001b[0;34m(self, x, h_0)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrx_encoder(x)\n\u001b[0;32m--> 125\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/ptls/nn/seq_encoder/rnn_encoder.py:140\u001b[0m, in \u001b[0;36mRnnEncoder.forward\u001b[0;34m(self, x, h_0)\u001b[0m\n\u001b[1;32m    138\u001b[0m out \u001b[38;5;241m=\u001b[39m PaddedBatch(out, x\u001b[38;5;241m.\u001b[39mseq_lens)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_reduce_sequence:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreducer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/ptls/nn/seq_step.py:32\u001b[0m, in \u001b[0;36mLastStepEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: PaddedBatch):\n\u001b[0;32m---> 32\u001b[0m     h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpayload[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mpayload)), [l \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mseq_lens]]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m~/miniconda/envs/env/lib/python3.9/site-packages/ptls/nn/seq_step.py:32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: PaddedBatch):\n\u001b[0;32m---> 32\u001b[0m     h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpayload[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mpayload)), [\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mseq_lens]]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train, val, test = preprocess(cfg_preprop)\n",
    "\n",
    "sequence_encoder = instantiate(cfg_encoder, is_reduce_sequence=True)\n",
    "sequence_encoder.load_state_dict(torch.load(backbone_path, map_location='cuda:0'))\n",
    "\n",
    "pooling_model = PoolingModel(train_data=train,\n",
    "                backbone=sequence_encoder,\n",
    "                backbone_embd_size=sequence_encoder.seq_encoder.hidden_size,\n",
    "                max_users_in_train_dataloader=300,\n",
    "                pooling_type=\"learnable_attention\",\n",
    "                min_seq_length=15,\n",
    "                max_seq_length=100,\n",
    "                max_embs_per_user=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model.set_pooling_type(\"exp_learnable_hawkes\", additional_config={\"hidden_sizes\": [100, 100]})# \"learnable_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling_model.set_pooling_type(\"symmetrical_attention\", additional_config={\"hidden_size\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling_model.set_pooling_type(\"learnable_hawkes\", additional_config={\"hawkes_nn\":{\"hidden_sizes\": [100, 100]},\n",
    "#                                                                       \"hawkes_time_nn\":{\"hidden_sizes\": [10, 10]}})# \"learnable_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling_model.set_pooling_type(\"kernel_attention\", additional_config={\"hidden_sizes\": [100, 100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing...\n",
      "Module creating...\n",
      "Freezing of weights...\n",
      "backbone.trx_encoder.embeddings.mcc_code.weight False\n",
      "backbone.trx_encoder.numerical_batch_norm.bn.weight False\n",
      "backbone.trx_encoder.numerical_batch_norm.bn.bias False\n",
      "backbone.seq_encoder.starter_h False\n",
      "backbone.seq_encoder.rnn.weight_ih_l0 False\n",
      "backbone.seq_encoder.rnn.weight_hh_l0 False\n",
      "backbone.seq_encoder.rnn.bias_ih_l0 False\n",
      "backbone.seq_encoder.rnn.bias_hh_l0 False\n",
      "learnable_attention_matrix.weight True\n",
      "learnable_attention_matrix.bias True\n",
      "hawkes_nn.layers.0.weight True\n",
      "hawkes_nn.layers.0.bias True\n",
      "hawkes_nn.layers.1.weight True\n",
      "hawkes_nn.layers.1.bias True\n",
      "hawkes_nn.layers.2.weight True\n",
      "hawkes_nn.layers.2.bias True\n",
      "Trainer creating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmakovka0911\u001b[0m (\u001b[33mskoltech_dl_course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/wandb/run-20240420_202411-b59o9j3x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skoltech_dl_course/app/runs/b59o9j3x' target=\"_blank\">lyric-sun-97</a></strong> to <a href='https://wandb.ai/skoltech_dl_course/app' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skoltech_dl_course/app' target=\"_blank\">https://wandb.ai/skoltech_dl_course/app</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skoltech_dl_course/app/runs/b59o9j3x' target=\"_blank\">https://wandb.ai/skoltech_dl_course/app/runs/b59o9j3x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder          | PoolingModel     | 5.7 M \n",
      "1 | decoder          | AbsDecoder       | 0     \n",
      "2 | amount_head      | Linear           | 2.0 K \n",
      "3 | mcc_head         | Linear           | 204 K \n",
      "4 | mcc_criterion    | CrossEntropyLoss | 0     \n",
      "5 | amount_criterion | MSELoss          | 0     \n",
      "6 | train_metrics    | ModuleDict       | 0     \n",
      "7 | val_metrics      | ModuleDict       | 0     \n",
      "8 | test_metrics     | ModuleDict       | 0     \n",
      "------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "4.3 M     Non-trainable params\n",
      "5.9 M     Total params\n",
      "11.778    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08c7ccf201941b89f88127fbdd19f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No `val_dataloader()` method defined to run `Trainer.validate`.\n"
     ]
    }
   ],
   "source": [
    "module = learn_pooling((train, val, test), pooling_model, learn_config_name=\"master\", seed=SEED, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model = module.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model.backbone.is_reduce_sequence = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.global_validation.global_validation_pipeline import embed_data, eval_embeddings\n",
    "seed_everything(SEED)\n",
    "# validation\n",
    "\n",
    "with initialize(config_path=\"../config\", version_base=None):\n",
    "    cfg = compose(config_name=\"master\")\n",
    "\n",
    "for val_type in cfg[\"validation\"]:\n",
    "\n",
    "    print(f\"-------Validation {val_type}-------\")\n",
    "    cfg_validation = cfg[\"validation\"][val_type]\n",
    "\n",
    "    if val_type == \"global_target\":\n",
    "\n",
    "        # get representations of sequences from train + val part\n",
    "        embeddings, targets = embed_data(pooling_model, train + val, **cfg_validation[\"embed_data\"])\n",
    "        N = len(embeddings)\n",
    "        indices = np.arange(N)\n",
    "\n",
    "        # get representations of sequences from test part\n",
    "        embeddings_test, targets_test = embed_data(\n",
    "            pooling_model,\n",
    "            test,\n",
    "            **cfg_validation[\"embed_data\"]\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        # bootstrap sample\n",
    "        bootstrap_inds = np.random.choice(indices, size=N, replace=True)\n",
    "        embeddings_train, targets_train = (\n",
    "            embeddings[bootstrap_inds],\n",
    "            targets[bootstrap_inds],\n",
    "        )\n",
    "    \n",
    "        # evaluate trained model\n",
    "        metrics = eval_embeddings(\n",
    "            embeddings_train,\n",
    "            targets_train,\n",
    "            embeddings_test,\n",
    "            targets_test,\n",
    "            cfg_validation[\"model\"],\n",
    "        )\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    else:\n",
    "        val_trainer = Trainer(**cfg_validation[\"trainer\"])#,\n",
    "                                #accelerator=\"gpu\")\n",
    "        \n",
    "        valid_model: LocalValidationModelBase = instantiate(\n",
    "        cfg_validation[\"module\"],\n",
    "            backbone=pooling_model,\n",
    "            backbone_embd_size=pooling_model.embedding_size\n",
    "        )\n",
    "\n",
    "        train_dataset = call(cfg_validation[\"dataset\"], data=train, deterministic=False)\n",
    "        val_dataset = call(cfg_validation[\"dataset\"], data=val, deterministic=True)\n",
    "        test_dataset = call(cfg_validation[\"dataset\"], data=test, deterministic=True)\n",
    "\n",
    "        datamodule: PtlsDataModule = instantiate(\n",
    "            cfg_validation[\"datamodule\"],\n",
    "            train_data=train_dataset,\n",
    "            valid_data=val_dataset,\n",
    "            test_data=test_dataset,\n",
    "            )\n",
    "\n",
    "        val_trainer.fit(valid_model, datamodule)\n",
    "        val_trainer.test(valid_model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "val_type = \"event_type\"\n",
    "\n",
    "print(f\"-------Validation {val_type}-------\")\n",
    "cfg_validation = cfg[\"validation\"][val_type]\n",
    "\n",
    "val_trainer = Trainer(**cfg_validation[\"trainer\"])#,\n",
    "                                #accelerator=\"gpu\")\n",
    "\n",
    "valid_model: LocalValidationModelBase = instantiate(\n",
    "cfg_validation[\"module\"],\n",
    "    backbone=pooling_model,\n",
    "    backbone_embd_size=pooling_model.embedding_size\n",
    ")\n",
    "\n",
    "train_dataset = call(cfg_validation[\"dataset\"], data=train, deterministic=False)\n",
    "val_dataset = call(cfg_validation[\"dataset\"], data=val, deterministic=True)\n",
    "test_dataset = call(cfg_validation[\"dataset\"], data=test, deterministic=True)\n",
    "\n",
    "datamodule: PtlsDataModule = instantiate(\n",
    "    cfg_validation[\"datamodule\"],\n",
    "    train_data=train_dataset,\n",
    "    valid_data=val_dataset,\n",
    "    test_data=test_dataset,\n",
    "    )\n",
    "\n",
    "val_trainer.fit(valid_model, datamodule)\n",
    "val_trainer.test(valid_model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pooling_model.learnable_attention_matrix.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(S.T @ S).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(S.T @ S, vmin = -0.03, vmax=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.T @ S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
