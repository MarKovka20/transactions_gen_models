{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate, call\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "from src.local_validation.local_validation_model import LocalValidationModelBase\n",
    "from src.preprocessing import preprocess\n",
    "\n",
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "from src.datasets.coles import CustomColesDataset\n",
    "from src.modules.coles import CustomCoLES\n",
    "\n",
    "from src.pooling import PoolingModel\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from src.utils.create_trainer import create_trainer\n",
    "from typing import Union\n",
    "from src.modules import Cotic, CustomCoLES, TS2Vec, VanillaAE\n",
    "from pytorch_lightning.utilities.model_helpers import is_overridden\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import learn_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"master\"\n",
    "backbone_path = \"saved_models/ar_default_1.pth\"#\"saved_models/churn/gpt/gpt_churn_1.pth\"\n",
    "SEED = 1\n",
    "\n",
    "with initialize(config_path=\"../config\", version_base=None):\n",
    "    cfg = compose(config_name=config_name)\n",
    "cfg_preprop = cfg[\"preprocessing\"]\n",
    "cfg_encoder = cfg[\"backbone\"][\"encoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(train)[\"user_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = preprocess(cfg_preprop)\n",
    "\n",
    "sequence_encoder = instantiate(cfg_encoder, is_reduce_sequence=True)\n",
    "sequence_encoder.load_state_dict(torch.load(backbone_path, map_location='cuda:0'))\n",
    "\n",
    "pooling_model = PoolingModel(train_data=train,\n",
    "                backbone=sequence_encoder,\n",
    "                backbone_embd_size=sequence_encoder.seq_encoder.hidden_size,\n",
    "                max_users_in_train_dataloader=300,\n",
    "                pooling_type=\"learnable_attention\",\n",
    "                min_seq_length=15,\n",
    "                max_seq_length=100,\n",
    "                max_embs_per_user=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model.set_pooling_type(\"exp_learnable_hawkes\", additional_config={\"hidden_sizes\": [100, 100]})# \"learnable_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling_model.set_pooling_type(\"symmetrical_attention\", additional_config={\"hidden_size\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling_model.set_pooling_type(\"learnable_hawkes\", additional_config={\"hawkes_nn\":{\"hidden_sizes\": [100, 100]},\n",
    "#                                                                       \"hawkes_time_nn\":{\"hidden_sizes\": [10, 10]}})# \"learnable_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling_model.set_pooling_type(\"kernel_attention\", additional_config={\"hidden_sizes\": [100, 100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing...\n",
      "Module creating...\n",
      "Freezing of weights...\n",
      "backbone.trx_encoder.embeddings.mcc_code.weight False\n",
      "backbone.trx_encoder.numerical_batch_norm.bn.weight False\n",
      "backbone.trx_encoder.numerical_batch_norm.bn.bias False\n",
      "backbone.seq_encoder.starter_h False\n",
      "backbone.seq_encoder.rnn.weight_ih_l0 False\n",
      "backbone.seq_encoder.rnn.weight_hh_l0 False\n",
      "backbone.seq_encoder.rnn.bias_ih_l0 False\n",
      "backbone.seq_encoder.rnn.bias_hh_l0 False\n",
      "learnable_attention_matrix.weight True\n",
      "learnable_attention_matrix.bias True\n",
      "hawkes_nn.layers.0.weight True\n",
      "hawkes_nn.layers.0.bias True\n",
      "hawkes_nn.layers.1.weight True\n",
      "hawkes_nn.layers.1.bias True\n",
      "hawkes_nn.layers.2.weight True\n",
      "hawkes_nn.layers.2.bias True\n",
      "Trainer creating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmakovka0911\u001b[0m (\u001b[33mskoltech_dl_course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/wandb/run-20240420_202411-b59o9j3x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skoltech_dl_course/app/runs/b59o9j3x' target=\"_blank\">lyric-sun-97</a></strong> to <a href='https://wandb.ai/skoltech_dl_course/app' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skoltech_dl_course/app' target=\"_blank\">https://wandb.ai/skoltech_dl_course/app</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skoltech_dl_course/app/runs/b59o9j3x' target=\"_blank\">https://wandb.ai/skoltech_dl_course/app/runs/b59o9j3x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder          | PoolingModel     | 5.7 M \n",
      "1 | decoder          | AbsDecoder       | 0     \n",
      "2 | amount_head      | Linear           | 2.0 K \n",
      "3 | mcc_head         | Linear           | 204 K \n",
      "4 | mcc_criterion    | CrossEntropyLoss | 0     \n",
      "5 | amount_criterion | MSELoss          | 0     \n",
      "6 | train_metrics    | ModuleDict       | 0     \n",
      "7 | val_metrics      | ModuleDict       | 0     \n",
      "8 | test_metrics     | ModuleDict       | 0     \n",
      "------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "4.3 M     Non-trainable params\n",
      "5.9 M     Total params\n",
      "11.778    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08c7ccf201941b89f88127fbdd19f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No `val_dataloader()` method defined to run `Trainer.validate`.\n"
     ]
    }
   ],
   "source": [
    "module = learn_pooling((train, val, test), pooling_model, learn_config_name=\"master\", seed=SEED, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model = module.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model.backbone.is_reduce_sequence = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.global_validation.global_validation_pipeline import embed_data, eval_embeddings\n",
    "seed_everything(SEED)\n",
    "# validation\n",
    "\n",
    "with initialize(config_path=\"../config\", version_base=None):\n",
    "    cfg = compose(config_name=\"master\")\n",
    "\n",
    "for val_type in cfg[\"validation\"]:\n",
    "\n",
    "    print(f\"-------Validation {val_type}-------\")\n",
    "    cfg_validation = cfg[\"validation\"][val_type]\n",
    "\n",
    "    if val_type == \"global_target\":\n",
    "\n",
    "        # get representations of sequences from train + val part\n",
    "        embeddings, targets = embed_data(pooling_model, train + val, **cfg_validation[\"embed_data\"])\n",
    "        N = len(embeddings)\n",
    "        indices = np.arange(N)\n",
    "\n",
    "        # get representations of sequences from test part\n",
    "        embeddings_test, targets_test = embed_data(\n",
    "            pooling_model,\n",
    "            test,\n",
    "            **cfg_validation[\"embed_data\"]\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        # bootstrap sample\n",
    "        bootstrap_inds = np.random.choice(indices, size=N, replace=True)\n",
    "        embeddings_train, targets_train = (\n",
    "            embeddings[bootstrap_inds],\n",
    "            targets[bootstrap_inds],\n",
    "        )\n",
    "    \n",
    "        # evaluate trained model\n",
    "        metrics = eval_embeddings(\n",
    "            embeddings_train,\n",
    "            targets_train,\n",
    "            embeddings_test,\n",
    "            targets_test,\n",
    "            cfg_validation[\"model\"],\n",
    "        )\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    else:\n",
    "        val_trainer = Trainer(**cfg_validation[\"trainer\"])#,\n",
    "                                #accelerator=\"gpu\")\n",
    "        \n",
    "        valid_model: LocalValidationModelBase = instantiate(\n",
    "        cfg_validation[\"module\"],\n",
    "            backbone=pooling_model,\n",
    "            backbone_embd_size=pooling_model.embedding_size\n",
    "        )\n",
    "\n",
    "        train_dataset = call(cfg_validation[\"dataset\"], data=train, deterministic=False)\n",
    "        val_dataset = call(cfg_validation[\"dataset\"], data=val, deterministic=True)\n",
    "        test_dataset = call(cfg_validation[\"dataset\"], data=test, deterministic=True)\n",
    "\n",
    "        datamodule: PtlsDataModule = instantiate(\n",
    "            cfg_validation[\"datamodule\"],\n",
    "            train_data=train_dataset,\n",
    "            valid_data=val_dataset,\n",
    "            test_data=test_dataset,\n",
    "            )\n",
    "\n",
    "        val_trainer.fit(valid_model, datamodule)\n",
    "        val_trainer.test(valid_model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
